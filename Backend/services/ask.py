# import os
# from typing import List

# from langchain_chroma import Chroma
# from langchain_classic.prompts import PromptTemplate
# from sqlalchemy.orm import Session

# from services.files import get_chroma_collection, get_embedding_model
# from models import FileModel


# # Lazy-load heavy components
# def _get_embedding_model():
#     return get_embedding_model()


# def _get_retriever(top_k: int = 5):
#     # Use langchain Chroma wrapper (persist directory is the same used by services.files)
#     embedding_model = _get_embedding_model()
#     vectorstore = Chroma(persist_directory="./chromadb_data", embedding_function=embedding_model)
#     return vectorstore.as_retriever(search_kwargs={"k": top_k})


# class MockLLMChain:
#     """
#     A tiny mock LLM chain that imitates the LLMChain.run API but returns a canned answer.

#     It's used only to maintain the LLMChain-like pattern while avoiding calls to any external LLM provider.
#     """
#     def __init__(self, template: PromptTemplate | None = None):
#         self.template = template

#     def run(self, **kwargs):
#         # Accepts context, question, or other input variables and returns a canned string.
#         # If you want richer mocked behavior, you can implement heuristics here.
#         return "(Mock) This is a simulated answer generated by MockLLMChain."


# def _get_llm_chain():
#     # Create and return a mock LLM chain always â€” we will not call external providers
#     prompt_template = PromptTemplate(
#         input_variables=["context", "question"],
#         template="Answer the question using the context below.\n\nContext:\n{context}\n\nQuestion: {question}"
#     )
#     return MockLLMChain(prompt_template)


# def _fetch_filenames_from_db(db: Session, file_ids: List[str]):
#     filenames = {}
#     if db is None:
#         return filenames
#     records = db.query(FileModel).filter(FileModel.id.in_(file_ids)).all()
#     for r in records:
#         filenames[r.id] = r.filename
#     return filenames


# def ask_question_chain(question: str, top_k: int = 5, mock: bool = False, db: Session | None = None):
#     """
#     Always-mocked LLM answer, while using LangChain retriever + a mock LLM chain to
#     follow the LLMChain pattern. We still return sources derived from retrieved docs
#     (file_id + filename when available).

#     Returns: {"answer": str, "sources": [{"file_id": str, "filename": str}]}
#     """
#     # 1) Retrieve relevant documents
#     retriever = _get_retriever(top_k=top_k)
#     # Different LangChain/VectorStore versions expose different APIs; try the public method then fallbacks
#     docs = None
#     try:
#         docs = retriever.get_relevant_documents(question)
#     except AttributeError:
#         # Fallback to older/internal method that may require a keyword-only run_manager
#         try:
#             docs = retriever._get_relevant_documents(question, run_manager=None)
#         except TypeError:
#             # Some connectors expose a different signature - try with only the query
#             try:
#                 docs = retriever._get_relevant_documents(question)
#             except Exception:
#                 docs = None
#     except Exception:
#         docs = None

#     # If we couldn't use the retriever, fall back to directly querying Chroma for documents/metadata
#     if docs is None:
#         try:
#             collection = get_chroma_collection()
#             embedding_model = _get_embedding_model()
#             query_emb = embedding_model.embed_query(question)
#             results = collection.query(query_embeddings=[query_emb], n_results=top_k, include=["documents", "metadatas", "ids"]) or {}
#             documents = results.get("documents", [[]])[0]
#             metadatas = results.get("metadatas", [[]])[0]
#             # Build docs as simple objects with the attributes `page_content` and `metadata`
#             from types import SimpleNamespace
#             docs = [SimpleNamespace(page_content=doc, metadata=meta) for doc, meta in zip(documents, metadatas)]
#         except Exception:
#             docs = []
#     if not docs:
#         return {"answer": "(Mock) I could not find any relevant information.", "sources": []}

#     # 2) Build context and run the mock LLM chain (we maintain the chain pattern)
#     context = "\n\n".join([d.page_content for d in docs])
#     llm_chain = _get_llm_chain()  # returns MockLLMChain
#     _ = llm_chain.run(context=context, question=question)  # we don't use the output for the final answer

#     # 3) Extract sources from document metadata
#     file_ids = []
#     for d in docs:
#         if hasattr(d, "metadata") and d.metadata.get("file_id"):
#             file_ids.append(d.metadata.get("file_id"))
#     # dedupe
#     file_ids = list(dict.fromkeys([fid for fid in file_ids if fid]))

#     # Resolve filenames from DB if provided
#     filenames = _fetch_filenames_from_db(db, file_ids)
#     sources = [{"file_id": fid, "filename": filenames.get(fid)} for fid in file_ids]

#     # 4) Return a mocked answer always
#     return {"answer": "(Mock) This is a simulated answer generated for development/testing purposes.", "sources": sources}


# services/qaservice.py
from typing import List
from types import SimpleNamespace
from sqlalchemy.orm import Session

from services.files import get_chroma_collection, get_embedding_model
from models import FileModel


def _fetch_filenames_from_db(db: Session, file_ids: List[str]):
    """Helper to get filenames from SQLite for given file IDs"""
    filenames = {}
    if db is None or not file_ids:
        return filenames
    records = db.query(FileModel).filter(FileModel.id.in_(file_ids)).all()
    for r in records:
        filenames[r.id] = r.filename
    return filenames


def retrieve_docs(question: str, top_k: int = 5):
    """
    Query Chroma collection directly to get documents and metadata.
    """
    collection = get_chroma_collection()
    embedding_model = get_embedding_model()
    query_emb = embedding_model.embed_query(question)

    results = collection.query(
        query_embeddings=[query_emb],
        n_results=top_k,
        include=["documents", "metadatas"]
    )

    # Convert to simple objects with page_content + metadata
    docs = []
    for doc_list, meta_list in zip(results["documents"], results["metadatas"]):
        for doc, meta in zip(doc_list, meta_list):
            docs.append(SimpleNamespace(page_content=doc, metadata=meta))
    return docs


def ask_question_chain(question: str, top_k: int = 5, db: Session | None = None):
    """
    Always-mocked LLM answer, while retrieving relevant chunks from Chroma.
    Returns:
        {
            "answer": str,
            "sources": [{"file_id": str, "filename": str}]
        }
    """
    # 1) Retrieve documents from Chroma
    docs = retrieve_docs(question, top_k=top_k)
    if not docs:
        return {"answer": "(Mock) No relevant documents found.", "sources": []}

    # 2) Build context (optional, only for mock)
    context = "\n\n".join([d.page_content for d in docs])

    # 3) Extract sources from document metadata
    file_ids = [d.metadata.get("file_id") for d in docs if d.metadata.get("file_id")]
    file_ids = list(dict.fromkeys(file_ids))  # dedupe

    # Resolve filenames from DB if provided
    filenames = _fetch_filenames_from_db(db, file_ids)
    sources = [{"file_id": fid, "filename": filenames.get(fid)} for fid in file_ids]

    # 4) Return a mocked answer with sources
    return {
        "answer": "(Mock) This is a simulated answer for testing purposes.",
        "sources": sources
    }
